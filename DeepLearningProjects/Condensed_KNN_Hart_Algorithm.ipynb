{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a2e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Assignment 1: k-NN Classification Algorithm & Application for Letter Recognition\n",
    "AIT-736, Instructor: Dr. Liao, Date: 9/15/2022\n",
    "Khanh Nguyen, Anisha Mou, Rohan Jonnakuti, Vishveshwar Kondala\n",
    "\n",
    "The code below using basic Python functions to build a Condensed k-NN algorithm from scratch using the Hart algorithm \n",
    "to implement the basic k-NN classification with a given Letter Recognition dataset for k_number = 1.  \n",
    "This dataset includes 20,000 examples. \n",
    "\n",
    "Here are the steps for Hart Algorithm:\n",
    "- Step 1: Initialize subset: Randomly select 10% samples from the training set as an initial subset and 90% as a test subset.\n",
    "- Step 2:   + Classify all remaining samples (test subset) using the initial subset (training set).\n",
    "            + Identify incorrectly classified samples and transfer to the subset.\n",
    "- Step 3: Re-run the knn-algorithm with the new training set and test set.              \n",
    "        \n",
    "*** The rest of the code is to compute accuracy score and confusion matrix with graph (similar to problem 1.1)\n",
    "'''\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "# %matplotlib inline\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#Changing path to working directory\n",
    "working_directory = os.getcwd()\n",
    "os.chdir(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78574a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly select 10% samples from the training set as an initial subset and 90% as a test subset\n",
      "Trainset size: (2000, 17)\n",
      "testset size: (1000, 17)\n",
      "Wall time: 39 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Split the Letter Recognition Dataset\n",
    "df = pd.read_csv('letter-recognition_data1.csv')\n",
    "\n",
    "#shuffling the dataset randomly\n",
    "# df = df.sample(frac = 1)\n",
    "\n",
    "def split_dataframe_by_position(df, splits):\n",
    "    \"\"\"\n",
    "Randomly select 10% samples from the training set as an initial subset \n",
    "and 90% as a test subset\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    index_to_split = 17999\n",
    "    start = 0\n",
    "    end = index_to_split\n",
    "    for split in range(splits):\n",
    "        temporary_df = df.iloc[start:end, :]\n",
    "        dataframes.append(temporary_df)\n",
    "        start += index_to_split\n",
    "        end += index_to_split\n",
    "    return dataframes\n",
    "split_dataframes = split_dataframe_by_position(df, 2)\n",
    "\n",
    "trainset = split_dataframes[1]\n",
    "testset1 = split_dataframes[0]\n",
    "\n",
    "def split_dataframe_by_position1(df, splits):\n",
    "    \"\"\"\n",
    "Randomly select 10% samples from the training set as an initial subset \n",
    "and 90% as a test subset\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    index_to_split = 1000\n",
    "    start = 0\n",
    "    end = index_to_split\n",
    "    for split in range(splits):\n",
    "        temporary_df = df.iloc[start:end, :]\n",
    "        dataframes.append(temporary_df)\n",
    "        start += index_to_split\n",
    "        end += index_to_split\n",
    "    return dataframes\n",
    "\n",
    "split_dataframes1 = split_dataframe_by_position1(testset1, 2)\n",
    "testset = split_dataframes1[0]\n",
    "\n",
    "# trainset.to_csv('trainset.csv')\n",
    "# testset.to_csv('testset.csv')\n",
    "print('Randomly select 10% samples from the training set as an initial subset and 90% as a test subset')\n",
    "print(\"Trainset size:\", trainset.shape)\n",
    "print(\"testset size:\", testset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bfc6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: [[5, 8, 6, 10, 9, 8, 9, 3, 2, 6, 8, 7, 6, 10, 6, 4, 'P'], [3, 5, 5, 3, 4, 7, 6, 3, 4, 9, 7, 8, 7, 6, 1, 8, 'M'], [8, 11, 7, 8, 5, 3, 11, 1, 3, 8, 10, 7, 6, 11, 2, 7, 'V'], [4, 5, 5, 4, 2, 4, 12, 3, 6, 12, 10, 4, 1, 11, 2, 5, 'Y'], [3, 4, 4, 5, 1, 7, 7, 4, 4, 7, 6, 8, 3, 8, 4, 8, 'X'], [1, 0, 2, 1, 0, 8, 9, 3, 2, 6, 12, 8, 2, 10, 0, 8, 'V'], [2, 5, 4, 4, 2, 10, 6, 2, 6, 12, 4, 9, 1, 6, 1, 7, 'J'], [2, 3, 2, 2, 1, 7, 7, 6, 3, 8, 6, 8, 2, 8, 2, 7, 'O'], [5, 6, 7, 4, 5, 9, 6, 2, 4, 9, 6, 7, 7, 6, 2, 7, 'M'], [5, 5, 6, 4, 2, 4, 12, 3, 3, 10, 11, 7, 2, 10, 1, 8, 'V']]\n",
      "Test1 sample:  [[5, 12, 3, 7, 2, 10, 5, 5, 4, 13, 3, 9, 2, 8, 4, 10], [4, 11, 6, 8, 6, 10, 6, 2, 6, 10, 3, 7, 3, 7, 3, 9], [7, 11, 6, 6, 3, 5, 9, 4, 6, 4, 4, 10, 6, 10, 2, 8], [2, 1, 3, 1, 1, 8, 6, 6, 6, 6, 5, 9, 1, 7, 5, 10], [4, 11, 5, 8, 3, 8, 8, 6, 9, 5, 6, 6, 0, 8, 9, 7], [4, 2, 5, 4, 4, 8, 7, 6, 6, 7, 6, 6, 2, 8, 7, 10], [1, 1, 3, 2, 1, 8, 2, 2, 2, 8, 2, 8, 1, 6, 2, 7], [2, 2, 4, 4, 2, 10, 6, 2, 6, 12, 4, 8, 1, 6, 1, 7], [11, 15, 13, 9, 7, 13, 2, 6, 2, 12, 1, 9, 8, 1, 1, 8], [3, 9, 5, 7, 4, 8, 7, 3, 8, 5, 6, 8, 2, 8, 6, 7]]\n",
      "test_subset is: [[[5, 12, 3, 7, 2, 10, 5, 5, 4, 13, 3, 9, 2, 8, 4, 10], 'I'], [[4, 11, 6, 8, 6, 10, 6, 2, 6, 10, 3, 7, 3, 7, 3, 9], 'D'], [[7, 11, 6, 6, 3, 5, 9, 4, 6, 4, 4, 10, 6, 10, 2, 8], 'N'], [[2, 1, 3, 1, 1, 8, 6, 6, 6, 6, 5, 9, 1, 7, 5, 10], 'G'], [[4, 11, 5, 8, 3, 8, 8, 6, 9, 5, 6, 6, 0, 8, 9, 7], 'S']]\n"
     ]
    }
   ],
   "source": [
    "''' First step of HART Algorithm:  initialize a subset for training and testing'''\n",
    "# Convert training and testing set to lists\n",
    "training = trainset.values.tolist()\n",
    "testing = testset.values.tolist()\n",
    "print(\"training set:\", training[:10])\n",
    "\n",
    "# Create a list exclude the labels (the column with letters) to test the algorithm\n",
    "test1 = testset.iloc[:,:-1] #all data except last column\n",
    "test1 = test1.values.tolist()\n",
    "print(\"Test1 sample: \", test1[:10])\n",
    "#Extract the actual letters from the test set (last column) and convert to a list for Accuracy calculation\n",
    "test_letters = testset.iloc[:, -1:]  #select only the last column\n",
    "test_letters = test_letters.values.tolist()\n",
    "actual_letters = list()\n",
    "for letter in test_letters:\n",
    "    actual_letters += letter\n",
    "# print(actual_letters)\n",
    "\n",
    "#Create the test_subset by joining test1 and its original labels (column) for comparison with predicted results\n",
    "test_subset =[]\n",
    "for a, b in zip(test1, actual_letters):\n",
    "    test_subset.append([a,b])\n",
    "    \n",
    "print(\"test_subset is:\", test_subset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5f26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make Predictions with k-nearest neighbors on the Letter Recognition Dataset\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "            # Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "# \treturn neighbors.pd.DataFrame()\n",
    "\treturn neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc88ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the KNN model initial subset is: 81.89999999999999\n",
      "The first classification of all remaining samples:  [[[5, 12, 3, 7, 2, 10, 5, 5, 4, 13, 3, 9, 2, 8, 4, 10], 'I'], [[4, 11, 6, 8, 6, 10, 6, 2, 6, 10, 3, 7, 3, 7, 3, 9], 'B'], [[7, 11, 6, 6, 3, 5, 9, 4, 6, 4, 4, 10, 6, 10, 2, 8], 'N'], [[2, 1, 3, 1, 1, 8, 6, 6, 6, 6, 5, 9, 1, 7, 5, 10], 'G'], [[4, 11, 5, 8, 3, 8, 8, 6, 9, 5, 6, 6, 0, 8, 9, 7], 'S'], [[4, 2, 5, 4, 4, 8, 7, 6, 6, 7, 6, 6, 2, 8, 7, 10], 'B'], [[1, 1, 3, 2, 1, 8, 2, 2, 2, 8, 2, 8, 1, 6, 2, 7], 'A'], [[2, 2, 4, 4, 2, 10, 6, 2, 6, 12, 4, 8, 1, 6, 1, 7], 'J'], [[11, 15, 13, 9, 7, 13, 2, 6, 2, 12, 1, 9, 8, 1, 1, 8], 'M'], [[3, 9, 5, 7, 4, 8, 7, 3, 8, 5, 6, 8, 2, 8, 6, 7], 'X']]\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"STEP 2 of HART algorithm: Classify all remaining samples using the subset\"\"\"\n",
    "\n",
    "# Make the first classification prediction with the initial training subset\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn [test_row, prediction]\n",
    "\n",
    "#Use k number of neighbors and predict the letters with the Euclidean-distance algorithms above\n",
    "num_neighbors = 1\n",
    "# predict the label\n",
    "predicted = list()\n",
    "for row in test1:\n",
    "    label = predict_classification(training, row, num_neighbors)\n",
    "    predicted.append(label)\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted): \n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "#get the Accuracy score for the nearest neighbor algorithm\n",
    "accuracy_score = accuracy_metric(test_subset, predicted)\n",
    "print(\"The accuracy score of the KNN model initial subset is:\", accuracy_score)\n",
    "print(\"The first classification of all remaining samples: \", predicted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29d5168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect classifieds to be removed and transferred: 181\n",
      "incorrect labels to be added to the initial subset: 181\n",
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\"Step2 and Step3 of HART algorithm: Extracting all the incorrect labels and transfer to the subset.\"\"\"\n",
    "def extract_incorrect_label(actual, predicted): \n",
    "    correct = 0\n",
    "    incorrect_labels = list()\n",
    "    removed_incorrects = list() \n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "        else: \n",
    "            incorrect_labels.append(predicted[i])\n",
    "            removed_incorrects.append(actual[i])\n",
    "    return incorrect_labels, removed_incorrects\n",
    "\n",
    "incorrect_labels, removed_incorrects = extract_incorrect_label(test_subset, predicted)\n",
    "print(\"incorrect classifieds to be removed and transferred:\", len(removed_incorrects))\n",
    "print(\"incorrect labels to be added to the initial subset:\", len(incorrect_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23b102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n",
      "Wall time: 8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[5, 12, 3, 7, 2, 10, 5, 5, 4, 13, 3, 9, 2, 8, 4, 10], 'I'],\n",
       " [[7, 11, 6, 6, 3, 5, 9, 4, 6, 4, 4, 10, 6, 10, 2, 8], 'N'],\n",
       " [[2, 1, 3, 1, 1, 8, 6, 6, 6, 6, 5, 9, 1, 7, 5, 10], 'G'],\n",
       " [[4, 11, 5, 8, 3, 8, 8, 6, 9, 5, 6, 6, 0, 8, 9, 7], 'S']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"FIRST RUN OF STEP 2 followed HART algorithm: Transfer (remove) incorrect classified labels from test subset\"\"\"\n",
    "new_test_subset = test_subset\n",
    "for element in removed_incorrects:\n",
    "    if element in test_subset:\n",
    "        new_test_subset.remove(element)\n",
    "print(len(new_test_subset))\n",
    "new_test_subset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c503de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [5, 12, 3, 7, 2, 10, 5, 5, 4, 13, 3, 9, 2, 8, ...\n",
      "1  [7, 11, 6, 6, 3, 5, 9, 4, 6, 4, 4, 10, 6, 10, ...\n",
      "2  [2, 1, 3, 1, 1, 8, 6, 6, 6, 6, 5, 9, 1, 7, 5, 10]\n",
      "3  [4, 11, 5, 8, 3, 8, 8, 6, 9, 5, 6, 6, 0, 8, 9, 7]\n",
      "4  [4, 2, 5, 4, 4, 8, 7, 6, 6, 7, 6, 6, 2, 8, 7, 10]\n",
      "   1\n",
      "0  I\n",
      "1  N\n",
      "2  G\n",
      "3  S\n",
      "4  B\n"
     ]
    }
   ],
   "source": [
    "new_test_subset_df = pd.DataFrame(new_test_subset)\n",
    "# print(new_test_subset_df.head())\n",
    "non_labels_df = new_test_subset_df.iloc[:,:-1] #all data except last column\n",
    "labels_only_df = new_test_subset_df.iloc[:,-1:]\n",
    "\n",
    "# labels_new_array = labels_only_df.to_numpy()\n",
    "print(non_labels_df.head())\n",
    "print(labels_only_df.head())\n",
    "# non_labels_df.to_csv('new_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4dc2508",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'new_test_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6840/2253931297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Apply the new test dataset after transferred out the incorrect classified samples\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new_test_df.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnew_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnew_test_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'new_test_df.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"Apply the new test dataset after transferred out the incorrect classified samples\"\"\"\n",
    "new_test = pd.read_csv('new_test_df.csv')\n",
    "new_test_df = new_test.values.tolist()\n",
    "new_test_df[:4]\n",
    "\n",
    "print(\"The count of new testing samples after transferred the incorrect classified samples: \",len(new_test_df))\n",
    "\n",
    "test2 = new_test.iloc[:,:-1] #all data except last column\n",
    "test2 = new_test.values.tolist()\n",
    "# print(\"Test1 sample: \", test1[:10])\n",
    "#Extract the actual letters from the test set (last column) and convert to a list for Accuracy calculation\n",
    "test_letters1 = new_test_subset_df.iloc[:, -1:]  #select only the last column\n",
    "test_letters1 = test_letters1.values.tolist()\n",
    "actual_letters1 = list()\n",
    "for letter in test_letters1:\n",
    "    actual_letters1 += letter\n",
    "\n",
    "actual_letters2 = actual_letters1[1:] \n",
    "print(len(actual_letters2))\n",
    "print(actual_letters[:5])\n",
    "#Create the test_subset by joining test1 and its original labels (column) for comparison with predicted results\n",
    "test_subset1 = []\n",
    "for a, b in zip(test2, actual_letters1):\n",
    "    test_subset.append([a,b])\n",
    "    \n",
    "# print(\"test_subset1 is:\", test_subset1[:5])\n",
    "# print(test2[:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df = pd.DataFrame(incorrect_labels)\n",
    "print(\"The total number of incorrect classified samples:\", len(incorrect_df))\n",
    "# incorrect_df.to_csv('incorrect_label.csv', index=False)\n",
    "# Manipulate the csv file to correct format for training subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step2 Hart Algorithm:  Transferred incorrect classified samples to the subset: new_training_subset'''\n",
    "new_train_df = pd.read_csv('incorrect_label.csv')\n",
    "new_train_subset = new_train_df.values.tolist()\n",
    "\n",
    "# new_train_subset = training\n",
    "new_training_subset = training + new_train_subset\n",
    "print(\"The count of new training subset after transferred the incorrect classified samples: \",len(new_training_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27521b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\" REPEATED STEP 2 of HART algorithm: Classify all remaining samples using the subset\"\"\"\n",
    "\n",
    "# Make the first classification prediction with the initial training subset\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    "\n",
    "#Use k number of neighbors and predict the letters with the Euclidean-distance algorithms above\n",
    "num_neighbors = 1\n",
    "# predict the label\n",
    "predicted1 = list()\n",
    "for row in new_test_df:\n",
    "    label = predict_classification(new_training_subset, row, num_neighbors)\n",
    "    predicted1.append(label)\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted): \n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "#get the Accuracy score for the nearest neighbor algorithm\n",
    "accuracy_score = accuracy_metric(actual_letters2, predicted1)\n",
    "print(\"The accuracy score of the Condensed Nearest Neighbor model after applying HART algorithm to the subset is:\", accuracy_score)\n",
    "print(\"The final classification of all remaining samples : \", predicted1[:10], \"and the count:\", len(predicted1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confusion Matrix without scikit-learn\n",
    "def confusionmatrix(actual, predicted, normalize = False):\n",
    "    \"\"\"\n",
    "    Generate a confusion matrix for multiple classification\n",
    "    @params:\n",
    "        actual      - a list of integers or strings for known classes\n",
    "        predicted   - a list of integers or strings for predicted classes\n",
    "        normalize   - optional boolean for matrix normalization\n",
    "    @return:\n",
    "        matrix      - a 2-dimensional list of pairwise counts\n",
    "    \"\"\"\n",
    "    unique = sorted(set(actual))\n",
    "    matrix = [[0 for _ in unique] for _ in unique]\n",
    "    imap   = {key: i for i, key in enumerate(unique)}\n",
    "    # Generate Confusion Matrix\n",
    "    for p, a in zip(predicted, actual):\n",
    "        matrix[imap[p]][imap[a]] += 1\n",
    "    # Matrix Normalization\n",
    "    if normalize:\n",
    "        sigma = sum([sum(matrix[imap[i]]) for i in unique])\n",
    "        matrix = [row for row in map(lambda i: list(map(lambda j: j / sigma, i)), matrix)]\n",
    "    return matrix\n",
    "\n",
    "cm = confusionmatrix(actual_letters2, predicted1)\n",
    "print(\"Confusion matrix for each letter: A-Z\")\n",
    "for line in cm:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df135476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.rc('axes.formatter', useoffset=False)\n",
    "plt.title('Confusion Matrix', size = 25)\n",
    "plt.ylabel('Actual Values', size = 25)\n",
    "plt.xlabel('Predicted Values', size = 25)\n",
    "# plt.gca().yaxis.set_major_formatter(plt.ScalarFormatter(useMathText=False))\n",
    "# plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4a739",
   "metadata": {},
   "source": [
    "## INTERPRETATION:\n",
    "The accuracy score of the Condensed Nearest Neighbor built from scratch is at aroun 81%, which is much lower than Task 2.1 at 94% but it is based on smaller training size.  Also the training subset was diluted by incorrect classified samples due to following the Hart Algorithm from the Lecture slide.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078797f",
   "metadata": {},
   "source": [
    "## REFERENCE\n",
    "Liao, L. (2022). Supervised Learning KNN algorithms.  George Mason University. Applied Machine Learning Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ed57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
